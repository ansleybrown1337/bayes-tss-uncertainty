---
title: "tss-uncertainty"
author: "A.J. Brown"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Bayesian Inference for TSS Measurement Uncertainty
By A.J. Brown
Agricultural Data Scientist
ansleybrown1337@gmail.com
[Personal Website](https://sites.google.com/view/ansleyjbrown/)

Description: This script is used to perform Bayesian inference on TSS using
the NIMBLE package in R. For more information please view the README.md file
in the root directory of [this repository](https://github.com/ansleybrown1337/bayes-tss-uncertainty).

## Table of Contents
1. [Load packages and data](#load-packages-and-data)
2. [Defining our Model](#defining-our-model)
3. [Setting the model up in NIMBLE](#setting-the-model-up-in-nimble)
4. [Posterior Predictive Sampling](#posterior-predictive-sampling)
5. [Running Monte Carlo Expectation Maximization](#running-monte-carlo-expectation-maximization)

## Load packages and data
```{r, warning=FALSE}
# Set working directory to repo parent folder
# Remember to make the knit button knit from current working directory
#setwd('../')
```

```{r, warning=FALSE, include=FALSE}

# Load packages
package.list <- c(
  'BayesianTools',  
  'nimble',
  'coda',
  'ggplot2',
  'dplyr',
  'tidyr'
  )
packageLoad <- function(packages){
  for (i in packages) {
    if (!require(i, character.only = TRUE)) {
      install.packages(i)
      library(i, character.only = TRUE)
    }
  }
}
packageLoad(package.list)
```

```{r}
# Load data
tss_df <- read.csv('../Example Data/TSS_Master_2023.csv')
```

```{r}
# Process data
  # separate data into two dataframes: standards and real data
std_list <- c('Stock Solution', 'DI')
std_df <- tss_df %>% filter(Sample_ID %in% std_list)
real_df <- tss_df %>% filter(!Sample_ID %in% std_list)

  # calculate error column for standards and center it
std_df <- std_df %>% mutate(
  tss.err = ifelse(Sample_ID == 'Stock Solution', 100-TSS_mg.L, 0-TSS_mg.L)
  )
  
  # change our categorical columns of interest into numbers
std_df$Sample_ID.num <- as.numeric(as.factor(std_df$Sample_ID))
std_df$Lab_technicians.num <- as.numeric(as.factor(std_df$Lab_technicians))

  # center our error
std_df$tss.err.ctr <- std_df$tss.err - mean(std_df$tss.err)
```

## Defining our model
We will use simple linear regression to estimate measurement error. I will
assume the following form:
$$
TSSerr_{i} = \alpha_k + \beta_{1, k} * person_k  + \epsilon
$$
Where:
- $TSSerr_{i}$ is the error in TSS measurement $i$ <br/>
- $\alpha_k$ is the intercept for person $k$ <br/>
- $\beta_{1, k}$ is the coefficient for the person effect <br/>
- $\epsilon$ is the error term for TSS measurement

## Setting the model up in NIMBLE 
### Step 1: Define the model
```{r, warning=FALSE}
# Step 1: Define the model
code <- nimbleCode({
  # Defining our priors
  alpha ~ dnorm(0, sd = 1) # intercept
  
  # Looping over the elements of person: they each get their own prior, and we will assume they are the same to start
  for(k in 1:3) {
    beta1[k] ~ dnorm(0, sd = 1)
  }
  
  sigma ~ dunif(0, 100) # prior for variance components based on Gelman (2006)

  # Defining our linear model
  for(i in 1:n) {
    err[i] ~ dnorm(alpha + beta1[person[i]], sd = sigma)
  }
})

constants <- list(
  n = nrow(std_df), 
  person = std_df$Lab_technicians.num
  )

data <- list(
  err = std_df$tss.err
  ) # also try using std_df$tss.err.ctr

inits <- list(
  alpha = mean(std_df$tss.err), 
  beta1 = rep(0, 3), 
  sigma = 1
  )

model <- nimbleModel(
  code, 
  constants = constants, 
  data = data, 
  inits = inits
  ) # build model
```
### Step 2: Build the MCMC
```{r}
#mcmcErr <- configureMCMC(model) # assign default samplers to nodes
# Step 2: Build the MCMC
TSSmcmc <- buildMCMC(model, enableWAIC = TRUE)
```

### Step 3: Compile the model and MCMC into C++ using Rtools
```{r, warning=FALSE}
# Step 3a: Compile the model
cTSSmodel <- compileNimble(model,showCompilerOutput = FALSE)
```

```{r, warning=FALSE}
# Step 3b: Compile the MCMC
cTSSmcmc <- compileNimble(TSSmcmc, project = model)
```

### Step 4: Run the MCMC
```{r}
# Step 4: Run the MCMC
time_baseline <- system.time(
  TSSresults <- runMCMC(cTSSmcmc,
                        niter=11000,
                        nburnin=1000, # change to 0 if you want to see burn in
                        WAIC=TRUE,
                        nchains=2)
  )
cat("Sampling time: ", time_baseline[3], "seconds.\n")
```

### Step 5: Extract the samples and WAIC
```{r}
# Step 5: Extract the samples and WAIC
  # Samples (default to 1st chain, 2nd is for convergence diagnostics)
samples <- TSSresults$samples$chain1
#colnames(samples)
summary(samples)
  # Watanabe-Akaike Information Criterion (WAIC): captures model fit
  # Log Pointwise Predictive Density (LPPD): captures model complexity
  # effective number of parameters in the model (pWAIC): balances previous two
  # The relationship: WAIC=−2×lppd+2×pWAIC
WAIC <- TSSresults$WAIC
cat("  \n\nWAIC Results:\n\n")  # Two spaces before \n\n
WAIC
```

### Step 6: Inspect Convergence
```{r, results='hide'}
# Step 6: Inspect Convergence
  # Using coda and BayesianTools packages
  # has to be pdf to capture all output
pdf('../Output/trace_density_plots.pdf')
plot(as.mcmc(samples))
dev.off()

png('../Output/correlation_plots.png')
correlationPlot(samples)
dev.off()

png('../Output/marginal_plot.png')
marginalPlot(samples)
dev.off()

# use gelman diag plots to compare chains
# see: https://theoreticalecology.wordpress.com/2011/12/09/mcmc-chain-analysis-and-convergence-diagnostics-with-coda-in-r/

combinedchains = mcmc.list(
  as.mcmc(TSSresults$samples$chain1), 
  as.mcmc(TSSresults$samples$chain2)
  )
plot(combinedchains)
gelman.diag(combinedchains)
gelman.plot(combinedchains)
```
The gelman.diag gives you the scale reduction factors for each parameter. A factor of 1 means that between variance and within chain variance are equal, larger values mean that there is still a notable difference between chains. Often, it is said that everything below 1.1 or so is OK, but note that this is more a rule of thumb. The gelman,plot shows you the development of the scale-reduction over time (chain steps), which is useful to see whether a low chain reduction is also stable (sometimes, the factors go down and then up again, as you will see). Also, note that for any real analysis, you have to make sure to discard any bias that arises from the starting point of your chain (burn-in), typical values here are a few 1000-10000 steps. The gelman plot is also a nice tool to see roughly where this point is, that is, from which point on the chains seem roughly converged.

Looking at our trace plots, we can see that our chains have converged. We can also see that our marginal density plots are approximately normal. Our correlation plots show that our chains are, for the most part, not correlated, which is also good. Given this, we can start to derive some insight from our model.

## Posterior Predictive Sampling
### Create a nimbleFunction to simulate new datasets
```{r}
# https://r-nimble.org/nimbleExamples/posterior_predictive.html
# Create a nimbleFunction to simulate new datasets
ppSamplerNF <- nimbleFunction(
          setup = function(model, mcmc) {
              dataNodes <- model$getNodeNames(dataOnly = TRUE)
              parentNodes <- model$getParents(dataNodes, stochOnly = TRUE)
              cat("Stochastic parents of data are:\n", paste(parentNodes, collapse = ','), ".\n\n")
              simNodes <- model$getDependencies(parentNodes, self = FALSE)
              vars <- mcmc$mvSamples$getVarNames()  # need ordering of variables in mvSamples / samples matrix
              cat("Using posterior samples of:\n", paste(vars, collapse = ','), ".\n\n")
              n <- length(model$expandNodeNames(dataNodes, returnScalarComponents = TRUE))
          },
          run = function(samples = double(2)) {
              nSamp <- dim(samples)[1]
              ppSamples <- matrix(nrow = nSamp, ncol = n)   
              for(i in 1:nSamp) {
                    values(model, vars) <<- samples[i, ]
                    model$simulate(simNodes, includeData = TRUE)
                    ppSamples[i, ] <- values(model, dataNodes)
              }
              returnType(double(2))       
              return(ppSamples)
          })

# Run the function on our data
## Create the sampler for this model and this MCMC.
ppSampler <- ppSamplerNF(model, TSSmcmc)
```
### Compile the nimbleFunction into C++
```{r}
cppSampler <- compileNimble(ppSampler, project = model)
```

### Run the nimbleFunction and get sample predictions
```{r}
ppSamples <- cppSampler$run(samples)
```

### Doing the posterior predictive checks
At this point, we can implement the check we want using our chosen discrepancy measures. We will use the mean, median, min, and max of the TSS error. We will also plot the posterior predictive distribution of the TSS error. We will do this for each of our four models.
```{r}
png('../Output/posterior_pred_plots.png')
# Define a list of statistics to calculate
statsList <- list(
  mean = list(func = mean, obs = mean(std_df$tss.err)),
  median = list(func = median, obs = median(std_df$tss.err)),
  min = list(func = min, obs = min(std_df$tss.err)),
  max = list(func = max, obs = max(std_df$tss.err))
)

# Setting up a 2x2 plotting area
par(mfrow = c(2, 2))

# Loop through each statistic
for (statName in names(statsList)) {
  # Calculate the statistic for each row in ppSamples
  ppStat <- apply(ppSamples, 1, statsList[[statName]]$func)
  
  # Create the histogram
  hist(ppStat,
       main = sprintf("Discrepancy = %s(TSS Error, ppm)", statName),
       xlab = sprintf("%s(modeled TSS, ppm)", statName),
       col = rainbow(length(statsList))[statName])
  
  # Add observed value line
  abline(v = statsList[[statName]]$obs, col = 'red')
}
# Resetting plotting area to default
par(mfrow = c(1, 1))

dev.off()
```
![Posterior](../Output/posterior_pred_plots.png)

Overall, it would appear that our model captures the TSS error well. The bulk of our predicted samples align closely with each observed value (i.e., the red line) for mean, median, and max. However, the observed min seems to be on the lower tail of our predicted distribution, indicating that our model may not simulate the lower error values as well. This could be due to the little number of observed samples at lower (i.e. more negative) values.  The bulk of our values lie near zero, making this harder to simulate without more data.

## Inspect beta differences and impacts on error
Now that we are fairly confident in our model's ability to simulate the TSS error, we can start to look at the differences in beta values and how they impact the TSS error. We will do this for each of our beta parameters.

### Effect of person ($\beta_1$) on TSS error
```{r}
# Get the beta1 samples from the sampler results
beta1_samples <- samples[ , grepl("beta1", colnames(samples))]

# Assuming beta1_samples is a matrix where each column represents a person
beta1_df <- as.data.frame(beta1_samples)
beta1_df$difference_AvAB <- beta1_df$`beta1[1]` - beta1_df$`beta1[2]`
beta1_df$difference_AvB <- beta1_df$`beta1[1]` - beta1_df$`beta1[3]`
beta1_df$difference_ABvB <- beta1_df$`beta1[2]` - beta1_df$`beta1[3]`

# Convert to long format for ggplot
beta1_long_df <- beta1_df %>%
  mutate(iteration = row_number()) %>%
  pivot_longer(-iteration, names_to = "person", values_to = "effect") %>%
  mutate(type = ifelse(grepl("difference", person), "difference", "beta"))

# Plot the posterior distributions of beta1
ax <- ggplot(beta1_long_df, aes(x = effect, color = person, linetype = type)) +
  geom_density() +  # Default color and fill
  scale_linetype_manual(values = c("solid", "dashed")) +  # Solid for beta, dashed for differences
  labs(title = "Posterior Distributions of Beta1 Coefficients",
       x = "Effect",
       y = "Density") +
  theme_minimal() +
  theme(legend.title = element_blank())
ax
ggsave('../Output/beta1_posterior_distributions.jpg', ax)

summary(beta1_df)
```
From here, let's calculate the lower (2.5%) and upper (97.5%) bounds of the 95% credible intervals for each column.

```{r}
# Calculate the 95% credible intervals for each column in beta1_df
credible_intervals <- apply(beta1_df, 2, function(x) {
  quantile(x, probs = c(0.025, 0.975))
})

# Convert to a more readable format (data frame)
credible_intervals_df <- as.data.frame(t(credible_intervals))
names(credible_intervals_df) <- c("Lower_2.5", "Upper_97.5")

# Add a column to check if zero is in the interval
credible_intervals_df$Contains_Zero <- with(credible_intervals_df, 
                                            Lower_2.5 <= 0 & Upper_97.5 >= 0)

# View the credible intervals
print(credible_intervals_df)
```
While we observe mean differences across the parameters that are not zero, the 95% credible intervals include zero, indicating substantial uncertainty about these effects.

### Intercept posterier distribution
```{r}
# Get the beta2 samples from the sampler results
alpha_samples <- samples[ , grepl("alpha", colnames(samples))]
# Assuming beta2_samples is a matrix where each column represents a solution
alpha_df <- as.data.frame(alpha_samples)

# Convert to long format for ggplot
alpha_long_df <- alpha_df %>%
  mutate(iteration = row_number()) %>%
  pivot_longer(-iteration, names_to = "solution", values_to = "effect")

# Plot the posterior distributions of beta2
ax2 <-ggplot(alpha_long_df, aes(x = effect, color = solution)) +
  geom_density(alpha = 0, fill = NA) +  # No fill under the curve
  labs(title = "Posterior Distributions of alpha Coefficients",
       x = "Effect",
       y = "Density") +
  theme_minimal() +
  scale_color_brewer(palette = "Set1") +
  theme(legend.title = element_blank())
ax2

ggsave('../Output/alpha_posterior_distributions.jpg', ax2)

summary(alpha_df)
```

```{r}
# posterior W distributions to show actual TSS error
samples_df <- as.data.frame(samples)
errA <- rnorm(1000, samples_df$alpha + samples_df$`beta1[1]`, samples_df$sigma)
errAB <- rnorm(1000, samples_df$alpha + samples_df$`beta1[2]`, samples_df$sigma)
errB <- rnorm(1000, samples_df$alpha + samples_df$`beta1[3]`, samples_df$sigma)

diff_AvAB <- errA - errAB
diff_AvB <- errA - errB
diff_ABvB <- errAB - errB

# Create two separate data frames for errors and differences
errors_df <- data.frame(
  value = c(errA, errAB, errB),
  group = factor(c(rep("Person A", length(errA)), 
                   rep("Person AB", length(errAB)), 
                   rep("Person B", length(errB))))
)

diffs_df <- data.frame(
  value = c(diff_AvAB, diff_AvB, diff_ABvB),
  group = factor(c(rep("diff_AvAB", length(diff_AvAB)),
                   rep("diff_AvB", length(diff_AvB)),
                   rep("diff_ABvB", length(diff_ABvB))))
)

# Plot errors
plot_errors <- ggplot(errors_df, aes(x = value, fill = group)) +
  geom_density(alpha = 0.25) +
  labs(x = "TSS error, ppm", y = "Density") +
  theme_minimal()

# Plot differences
plot_diffs <- ggplot(diffs_df, aes(x = value, fill = group)) +
  geom_density(alpha = 0.25) +
  labs(x = "Difference in TSS error, ppm", y = "Density") +
  theme_minimal()

# Save plots as JPEG
ggsave('../Output/TSSerror_distributions.jpg', plot_errors, device = "jpeg")
ggsave('../Output/PersonDifference_distributions.jpg', plot_diffs, device = "jpeg")

# Create a list containing all the vectors
all_vars <- list(errA = errA, errAB = errAB, errB = errB, 
                 diff_AvAB = diff_AvAB, diff_AvB = diff_AvB, diff_ABvB = diff_ABvB)

# Loop through the list and print summary for each vector
for (var_name in names(all_vars)) {
  cat("Summary for", var_name, ":\n")
  print(summary(all_vars[[var_name]]))
  cat("\n") # For better readability
}
```
In practice, our analysis suggests there were average differences in TSS readings between lab technicians. However, these averages do not decisively indicate that any one technician consistently had higher or lower error rates than others. It's important to note that while we observed these differences, the variability in the data is such that the actual error associated with each technician could plausibly be zero. Essentially, what this means is that while we can identify trends in the data, there is significant uncertainty around these estimates. This analysis provides us with a quantified measure of this uncertainty for each technician’s TSS readings. For practical purposes, this means that while we can speculate about differences in performance among technicians, we should be cautious about making definitive judgments based on these results alone.

## Running Monte Carlo Expectation Maximization
```{r}
# TODO: Still working on this...
# model2 <- model$newModel()
# 
# box = list(list(c("alpha","beta"), c(0, Inf)))
# 
# pumpMCEM <- buildMCEM(model = pump2, latentNodes = "theta[1:10]",
#                       boxConstraints = box)
# pumpMLE <- pumpMCEM$run()
```

